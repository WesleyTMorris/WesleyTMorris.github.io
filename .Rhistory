const page = await browser.newPage();
const url = "%s";
await page.goto(url, { waitUntil: "networkidle2" }); // Wait until the network is idle
// Find the iframe element
const iframeElement = await page.waitForSelector("#icims_content_iframe");
const frame = await iframeElement.contentFrame();
// Get the HTML content of the iframe
const iframeHTML = await frame.content();
// Save the content to a file
const fs = require("fs");
fs.writeFileSync("%s", iframeHTML);
await browser.close();
})();', url, path)
# Specify the file path where you want to save the JavaScript code
js_file_path <- "/Users/wesleymorris/my-puppeteer-scraper/scraper_Puppeteer.js"
# Write the JavaScript code to the file
writeLines(js_code, con = js_file_path)
# Confirm that the file has been created
file.exists(js_file_path)
# Running the JavaScript code using Puppeteer
system("cd /Users/wesleymorris/my-puppeteer-scraper/")  # Move to the directory
system(paste("node", js_file_path), wait = TRUE)  # Execute the JavaScript file
district_html <- read_html(path)
job_url <- district_html %>%
html_elements("body > div.iCIMS_MainWrapper.iCIMS_ListingsPage > div.container-fluid.iCIMS_JobsTable > div div a") %>%
html_attr("href")
building <- list()  # Initialize a list to store data frames
prefixes <- c("Location Name", "Address", "Category", "Position Type", "Pay Grade Compensation", "Posted Date", "Availability Date")
for (x in job_url) {
job_html <- read_html(x)
filename <- paste(new_folder_path, "/html/", lea_name, "_", wesley, ".html", sep = "")
writeLines(as.character(job_html), filename)
wesley<- wesley+1
extracted_data <- list()
title <- job_html %>%
html_elements("#iCIMS_Header > h1") %>%
html_text2()
# Convert 'Title' values to characters explicitly
extracted_data[["Title"]] <- as.character(title)
additional <- job_html %>%
html_elements("body > div.iCIMS_MainWrapper.iCIMS_JobPage.iCIMS_Desktop > div.iCIMS_JobContainer > div.iCIMS_JobContent > div.container-fluid.iCIMS_JobsTable > div > div.col-xs-12.additionalFields > dl div") %>%
html_text2()
for (line in additional) {
for (prefix in prefixes) {
if (grepl(paste0("^", prefix), line)) {
value <- gsub(paste0("^", prefix, "\n"), "", line)  # Extract value after prefix
extracted_data[[prefix]] <- value
}
}
}
# Create a data frame for the current job entry
formatted_data <- data.frame(matrix(ncol = length(prefixes) + 1, nrow = 1))  # +1 for 'Title' column
colnames(formatted_data) <- c("Title", prefixes)
# Assign values to corresponding columns
for (col in names(formatted_data)) {
formatted_data[[col]] <- extracted_data[[col]]
}
# Append the data frame to the list
building[[length(building) + 1]] <- formatted_data
}
# Combine all data frames from the list into a single dataframe 'result' using bind_rows
building_df <- dplyr::bind_rows(building)
# Generate a unique filename based on the counter
filename <- paste(new_folder_path, "/data_", lea_name, ".csv", sep = "")
# Save the data frame as a CSV file
write.csv(building_df, file = filename, row.names = FALSE)
counter <- counter+1
}
}
district_URL <- read.csv("/Users/wesleymorris/Desktop/EPP Geography/Links to District Job Vacancy Sites.csv")
wesley <- 1601
# Scenario 5-2 - Wake County
{
# Starting an empty data frame to build onto
Data_job_openings <- data.frame()
# Define the base directory path
base_directory <- paste(new_folder_path, "html/", sep = "/")
counter <- 1
for (x in district_URL$URL[district_URL$Platform_code == 15]) {
url <- x
lea_name <- district_URL$lea_name[district_URL$Platform_code == 15][counter]
# Specify the path to save the HTML file with the full directory path
path <- paste(base_directory, lea_name, ".html", sep = "")
# Define the Puppeteer JavaScript code template with placeholders for URL and path
js_code <- sprintf('
const puppeteer = require("puppeteer");
(async () => {
const browser = await puppeteer.launch();
const page = await browser.newPage();
const url = "%s";
await page.goto(url, { waitUntil: "networkidle2" }); // Wait until the network is idle
// Find the iframe element
const iframeElement = await page.waitForSelector("#icims_content_iframe");
const frame = await iframeElement.contentFrame();
// Get the HTML content of the iframe
const iframeHTML = await frame.content();
// Save the content to a file
const fs = require("fs");
fs.writeFileSync("%s", iframeHTML);
await browser.close();
})();', url, path)
# Specify the file path where you want to save the JavaScript code
js_file_path <- "/Users/wesleymorris/my-puppeteer-scraper/scraper_Puppeteer.js"
# Write the JavaScript code to the file
writeLines(js_code, con = js_file_path)
# Confirm that the file has been created
file.exists(js_file_path)
# Running the JavaScript code using Puppeteer
system("cd /Users/wesleymorris/my-puppeteer-scraper/")  # Move to the directory
system(paste("node", js_file_path), wait = TRUE)  # Execute the JavaScript file
district_html <- read_html(path)
job_url <- district_html %>%
html_elements("body > div.iCIMS_MainWrapper.iCIMS_ListingsPage > div.container-fluid.iCIMS_JobsTable > div div a") %>%
html_attr("href")
building <- list()  # Initialize a list to store data frames
prefixes <- c("Location Name", "Address", "Category", "Position Type", "Pay Grade Compensation", "Posted Date", "Availability Date")
for (x in job_url) {
job_html <- read_html(x)
filename <- paste(new_folder_path, "/html/", lea_name, "_", wesley, ".html", sep = "")
writeLines(as.character(job_html), filename)
wesley<- wesley+1
extracted_data <- list()
title <- job_html %>%
html_elements("#iCIMS_Header > h1") %>%
html_text2()
# Convert 'Title' values to characters explicitly
extracted_data[["Title"]] <- as.character(title)
additional <- job_html %>%
html_elements("body > div.iCIMS_MainWrapper.iCIMS_JobPage.iCIMS_Desktop > div.iCIMS_JobContainer > div.iCIMS_JobContent > div.container-fluid.iCIMS_JobsTable > div > div.col-xs-12.additionalFields > dl div") %>%
html_text2()
for (line in additional) {
for (prefix in prefixes) {
if (grepl(paste0("^", prefix), line)) {
value <- gsub(paste0("^", prefix, "\n"), "", line)  # Extract value after prefix
extracted_data[[prefix]] <- value
}
}
}
# Create a data frame for the current job entry
formatted_data <- data.frame(matrix(ncol = length(prefixes) + 1, nrow = 1))  # +1 for 'Title' column
colnames(formatted_data) <- c("Title", prefixes)
# Assign values to corresponding columns
for (col in names(formatted_data)) {
formatted_data[[col]] <- extracted_data[[col]]
}
# Append the data frame to the list
building[[length(building) + 1]] <- formatted_data
}
# Combine all data frames from the list into a single dataframe 'result' using bind_rows
building_df <- dplyr::bind_rows(building)
# Generate a unique filename based on the counter
filename <- paste(new_folder_path, "/data_", lea_name, ".csv", sep = "")
# Save the data frame as a CSV file
write.csv(building_df, file = filename, row.names = FALSE)
counter <- counter+1
}
}
district_URL <- read.csv("/Users/wesleymorris/Desktop/EPP Geography/Links to District Job Vacancy Sites.csv")
wesley <- 1601
# Scenario 5-2 - Wake County
{
# Starting an empty data frame to build onto
Data_job_openings <- data.frame()
# Define the base directory path
base_directory <- paste(new_folder_path, "html/", sep = "/")
counter <- 1
for (x in district_URL$URL[district_URL$Platform_code == 15]) {
url <- x
lea_name <- district_URL$lea_name[district_URL$Platform_code == 15][counter]
# Specify the path to save the HTML file with the full directory path
path <- paste(base_directory, lea_name, ".html", sep = "")
# Define the Puppeteer JavaScript code template with placeholders for URL and path
js_code <- sprintf('
const puppeteer = require("puppeteer");
(async () => {
const browser = await puppeteer.launch();
const page = await browser.newPage();
const url = "%s";
await page.goto(url, { waitUntil: "networkidle2" }); // Wait until the network is idle
// Find the iframe element
const iframeElement = await page.waitForSelector("#icims_content_iframe");
const frame = await iframeElement.contentFrame();
// Get the HTML content of the iframe
const iframeHTML = await frame.content();
// Save the content to a file
const fs = require("fs");
fs.writeFileSync("%s", iframeHTML);
await browser.close();
})();', url, path)
# Specify the file path where you want to save the JavaScript code
js_file_path <- "/Users/wesleymorris/my-puppeteer-scraper/scraper_Puppeteer.js"
# Write the JavaScript code to the file
writeLines(js_code, con = js_file_path)
# Confirm that the file has been created
file.exists(js_file_path)
# Running the JavaScript code using Puppeteer
system("cd /Users/wesleymorris/my-puppeteer-scraper/")  # Move to the directory
system(paste("node", js_file_path), wait = TRUE)  # Execute the JavaScript file
district_html <- read_html(path)
job_url <- district_html %>%
html_elements("body > div.iCIMS_MainWrapper.iCIMS_ListingsPage > div.container-fluid.iCIMS_JobsTable > div div a") %>%
html_attr("href")
building <- list()  # Initialize a list to store data frames
prefixes <- c("Location Name", "Address", "Category", "Position Type", "Pay Grade Compensation", "Posted Date", "Availability Date")
for (x in job_url) {
job_html <- read_html(x)
filename <- paste(new_folder_path, "/html/", lea_name, "_", wesley, ".html", sep = "")
writeLines(as.character(job_html), filename)
wesley<- wesley+1
extracted_data <- list()
title <- job_html %>%
html_elements("#iCIMS_Header > h1") %>%
html_text2()
# Convert 'Title' values to characters explicitly
extracted_data[["Title"]] <- as.character(title)
additional <- job_html %>%
html_elements("body > div.iCIMS_MainWrapper.iCIMS_JobPage.iCIMS_Desktop > div.iCIMS_JobContainer > div.iCIMS_JobContent > div.container-fluid.iCIMS_JobsTable > div > div.col-xs-12.additionalFields > dl div") %>%
html_text2()
for (line in additional) {
for (prefix in prefixes) {
if (grepl(paste0("^", prefix), line)) {
value <- gsub(paste0("^", prefix, "\n"), "", line)  # Extract value after prefix
extracted_data[[prefix]] <- value
}
}
}
# Create a data frame for the current job entry
formatted_data <- data.frame(matrix(ncol = length(prefixes) + 1, nrow = 1))  # +1 for 'Title' column
colnames(formatted_data) <- c("Title", prefixes)
# Assign values to corresponding columns
for (col in names(formatted_data)) {
formatted_data[[col]] <- extracted_data[[col]]
}
# Append the data frame to the list
building[[length(building) + 1]] <- formatted_data
}
# Combine all data frames from the list into a single dataframe 'result' using bind_rows
building_df <- dplyr::bind_rows(building)
# Generate a unique filename based on the counter
filename <- paste(new_folder_path, "/data_", lea_name, ".csv", sep = "")
# Save the data frame as a CSV file
write.csv(building_df, file = filename, row.names = FALSE)
counter <- counter+1
}
}
district_URL <- read.csv("/Users/wesleymorris/Desktop/EPP Geography/Links to District Job Vacancy Sites.csv")
wesley <- 1701
# Scenario 5-2 - Wake County
{
# Starting an empty data frame to build onto
Data_job_openings <- data.frame()
# Define the base directory path
base_directory <- paste(new_folder_path, "html/", sep = "/")
counter <- 1
for (x in district_URL$URL[district_URL$Platform_code == 15]) {
url <- x
lea_name <- district_URL$lea_name[district_URL$Platform_code == 15][counter]
# Specify the path to save the HTML file with the full directory path
path <- paste(base_directory, lea_name, ".html", sep = "")
# Define the Puppeteer JavaScript code template with placeholders for URL and path
js_code <- sprintf('
const puppeteer = require("puppeteer");
(async () => {
const browser = await puppeteer.launch();
const page = await browser.newPage();
const url = "%s";
await page.goto(url, { waitUntil: "networkidle2" }); // Wait until the network is idle
// Find the iframe element
const iframeElement = await page.waitForSelector("#icims_content_iframe");
const frame = await iframeElement.contentFrame();
// Get the HTML content of the iframe
const iframeHTML = await frame.content();
// Save the content to a file
const fs = require("fs");
fs.writeFileSync("%s", iframeHTML);
await browser.close();
})();', url, path)
# Specify the file path where you want to save the JavaScript code
js_file_path <- "/Users/wesleymorris/my-puppeteer-scraper/scraper_Puppeteer.js"
# Write the JavaScript code to the file
writeLines(js_code, con = js_file_path)
# Confirm that the file has been created
file.exists(js_file_path)
# Running the JavaScript code using Puppeteer
system("cd /Users/wesleymorris/my-puppeteer-scraper/")  # Move to the directory
system(paste("node", js_file_path), wait = TRUE)  # Execute the JavaScript file
district_html <- read_html(path)
job_url <- district_html %>%
html_elements("body > div.iCIMS_MainWrapper.iCIMS_ListingsPage > div.container-fluid.iCIMS_JobsTable > div div a") %>%
html_attr("href")
building <- list()  # Initialize a list to store data frames
prefixes <- c("Location Name", "Address", "Category", "Position Type", "Pay Grade Compensation", "Posted Date", "Availability Date")
for (x in job_url) {
job_html <- read_html(x)
filename <- paste(new_folder_path, "/html/", lea_name, "_", wesley, ".html", sep = "")
writeLines(as.character(job_html), filename)
wesley<- wesley+1
extracted_data <- list()
title <- job_html %>%
html_elements("#iCIMS_Header > h1") %>%
html_text2()
# Convert 'Title' values to characters explicitly
extracted_data[["Title"]] <- as.character(title)
additional <- job_html %>%
html_elements("body > div.iCIMS_MainWrapper.iCIMS_JobPage.iCIMS_Desktop > div.iCIMS_JobContainer > div.iCIMS_JobContent > div.container-fluid.iCIMS_JobsTable > div > div.col-xs-12.additionalFields > dl div") %>%
html_text2()
for (line in additional) {
for (prefix in prefixes) {
if (grepl(paste0("^", prefix), line)) {
value <- gsub(paste0("^", prefix, "\n"), "", line)  # Extract value after prefix
extracted_data[[prefix]] <- value
}
}
}
# Create a data frame for the current job entry
formatted_data <- data.frame(matrix(ncol = length(prefixes) + 1, nrow = 1))  # +1 for 'Title' column
colnames(formatted_data) <- c("Title", prefixes)
# Assign values to corresponding columns
for (col in names(formatted_data)) {
formatted_data[[col]] <- extracted_data[[col]]
}
# Append the data frame to the list
building[[length(building) + 1]] <- formatted_data
}
# Combine all data frames from the list into a single dataframe 'result' using bind_rows
building_df <- dplyr::bind_rows(building)
# Generate a unique filename based on the counter
filename <- paste(new_folder_path, "/data_", lea_name, ".csv", sep = "")
# Save the data frame as a CSV file
write.csv(building_df, file = filename, row.names = FALSE)
counter <- counter+1
}
}
district_URL <- read.csv("/Users/wesleymorris/Desktop/EPP Geography/Links to District Job Vacancy Sites.csv")
wesley <- 1701
# Scenario 5-2 - Wake County
{
# Starting an empty data frame to build onto
Data_job_openings <- data.frame()
# Define the base directory path
base_directory <- paste(new_folder_path, "html/", sep = "/")
counter <- 1
for (x in district_URL$URL[district_URL$Platform_code == 15]) {
url <- x
lea_name <- district_URL$lea_name[district_URL$Platform_code == 15][counter]
# Specify the path to save the HTML file with the full directory path
path <- paste(base_directory, lea_name, ".html", sep = "")
# Define the Puppeteer JavaScript code template with placeholders for URL and path
js_code <- sprintf('
const puppeteer = require("puppeteer");
(async () => {
const browser = await puppeteer.launch();
const page = await browser.newPage();
const url = "%s";
await page.goto(url, { waitUntil: "networkidle2" }); // Wait until the network is idle
// Find the iframe element
const iframeElement = await page.waitForSelector("#icims_content_iframe");
const frame = await iframeElement.contentFrame();
// Get the HTML content of the iframe
const iframeHTML = await frame.content();
// Save the content to a file
const fs = require("fs");
fs.writeFileSync("%s", iframeHTML);
await browser.close();
})();', url, path)
# Specify the file path where you want to save the JavaScript code
js_file_path <- "/Users/wesleymorris/my-puppeteer-scraper/scraper_Puppeteer.js"
# Write the JavaScript code to the file
writeLines(js_code, con = js_file_path)
# Confirm that the file has been created
file.exists(js_file_path)
# Running the JavaScript code using Puppeteer
system("cd /Users/wesleymorris/my-puppeteer-scraper/")  # Move to the directory
system(paste("node", js_file_path), wait = TRUE)  # Execute the JavaScript file
district_html <- read_html(path)
job_url <- district_html %>%
html_elements("body > div.iCIMS_MainWrapper.iCIMS_ListingsPage > div.container-fluid.iCIMS_JobsTable > div div a") %>%
html_attr("href")
building <- list()  # Initialize a list to store data frames
prefixes <- c("Location Name", "Address", "Category", "Position Type", "Pay Grade Compensation", "Posted Date", "Availability Date")
for (x in job_url) {
job_html <- read_html(x)
filename <- paste(new_folder_path, "/html/", lea_name, "_", wesley, ".html", sep = "")
writeLines(as.character(job_html), filename)
wesley<- wesley+1
extracted_data <- list()
title <- job_html %>%
html_elements("#iCIMS_Header > h1") %>%
html_text2()
# Convert 'Title' values to characters explicitly
extracted_data[["Title"]] <- as.character(title)
additional <- job_html %>%
html_elements("body > div.iCIMS_MainWrapper.iCIMS_JobPage.iCIMS_Desktop > div.iCIMS_JobContainer > div.iCIMS_JobContent > div.container-fluid.iCIMS_JobsTable > div > div.col-xs-12.additionalFields > dl div") %>%
html_text2()
for (line in additional) {
for (prefix in prefixes) {
if (grepl(paste0("^", prefix), line)) {
value <- gsub(paste0("^", prefix, "\n"), "", line)  # Extract value after prefix
extracted_data[[prefix]] <- value
}
}
}
# Create a data frame for the current job entry
formatted_data <- data.frame(matrix(ncol = length(prefixes) + 1, nrow = 1))  # +1 for 'Title' column
colnames(formatted_data) <- c("Title", prefixes)
# Assign values to corresponding columns
for (col in names(formatted_data)) {
formatted_data[[col]] <- extracted_data[[col]]
}
# Append the data frame to the list
building[[length(building) + 1]] <- formatted_data
}
# Combine all data frames from the list into a single dataframe 'result' using bind_rows
building_df <- dplyr::bind_rows(building)
# Generate a unique filename based on the counter
filename <- paste(new_folder_path, "/data_", lea_name, ".csv", sep = "")
# Save the data frame as a CSV file
write.csv(building_df, file = filename, row.names = FALSE)
counter <- counter+1
}
}
district_URL <- read.csv("/Users/wesleymorris/Desktop/EPP Geography/Links to District Job Vacancy Sites.csv")
wesley <- 1701
# Scenario 5-2 - Wake County
{
# Starting an empty data frame to build onto
Data_job_openings <- data.frame()
# Define the base directory path
base_directory <- paste(new_folder_path, "html/", sep = "/")
counter <- 1
for (x in district_URL$URL[district_URL$Platform_code == 15]) {
url <- x
lea_name <- district_URL$lea_name[district_URL$Platform_code == 15][counter]
# Specify the path to save the HTML file with the full directory path
path <- paste(base_directory, lea_name, ".html", sep = "")
# Define the Puppeteer JavaScript code template with placeholders for URL and path
js_code <- sprintf('
const puppeteer = require("puppeteer");
(async () => {
const browser = await puppeteer.launch();
const page = await browser.newPage();
const url = "%s";
await page.goto(url, { waitUntil: "networkidle2" }); // Wait until the network is idle
// Find the iframe element
const iframeElement = await page.waitForSelector("#icims_content_iframe");
const frame = await iframeElement.contentFrame();
// Get the HTML content of the iframe
const iframeHTML = await frame.content();
// Save the content to a file
const fs = require("fs");
fs.writeFileSync("%s", iframeHTML);
await browser.close();
})();', url, path)
# Specify the file path where you want to save the JavaScript code
js_file_path <- "/Users/wesleymorris/my-puppeteer-scraper/scraper_Puppeteer.js"
# Write the JavaScript code to the file
writeLines(js_code, con = js_file_path)
# Confirm that the file has been created
file.exists(js_file_path)
# Running the JavaScript code using Puppeteer
system("cd /Users/wesleymorris/my-puppeteer-scraper/")  # Move to the directory
system(paste("node", js_file_path), wait = TRUE)  # Execute the JavaScript file
district_html <- read_html(path)
job_url <- district_html %>%
html_elements("body > div.iCIMS_MainWrapper.iCIMS_ListingsPage > div.container-fluid.iCIMS_JobsTable > div div a") %>%
html_attr("href")
building <- list()  # Initialize a list to store data frames
prefixes <- c("Location Name", "Address", "Category", "Position Type", "Pay Grade Compensation", "Posted Date", "Availability Date")
for (x in job_url) {
job_html <- read_html(x)
filename <- paste(new_folder_path, "/html/", lea_name, "_", wesley, ".html", sep = "")
writeLines(as.character(job_html), filename)
wesley<- wesley+1
extracted_data <- list()
title <- job_html %>%
html_elements("#iCIMS_Header > h1") %>%
html_text2()
# Convert 'Title' values to characters explicitly
extracted_data[["Title"]] <- as.character(title)
additional <- job_html %>%
html_elements("body > div.iCIMS_MainWrapper.iCIMS_JobPage.iCIMS_Desktop > div.iCIMS_JobContainer > div.iCIMS_JobContent > div.container-fluid.iCIMS_JobsTable > div > div.col-xs-12.additionalFields > dl div") %>%
html_text2()
for (line in additional) {
for (prefix in prefixes) {
if (grepl(paste0("^", prefix), line)) {
value <- gsub(paste0("^", prefix, "\n"), "", line)  # Extract value after prefix
extracted_data[[prefix]] <- value
}
}
}
# Create a data frame for the current job entry
formatted_data <- data.frame(matrix(ncol = length(prefixes) + 1, nrow = 1))  # +1 for 'Title' column
colnames(formatted_data) <- c("Title", prefixes)
# Assign values to corresponding columns
for (col in names(formatted_data)) {
formatted_data[[col]] <- extracted_data[[col]]
}
# Append the data frame to the list
building[[length(building) + 1]] <- formatted_data
}
# Combine all data frames from the list into a single dataframe 'result' using bind_rows
building_df <- dplyr::bind_rows(building)
# Generate a unique filename based on the counter
filename <- paste(new_folder_path, "/data_", lea_name, ".csv", sep = "")
# Save the data frame as a CSV file
write.csv(building_df, file = filename, row.names = FALSE)
counter <- counter+1
}
}
quanto render
quarto render
quarto render
